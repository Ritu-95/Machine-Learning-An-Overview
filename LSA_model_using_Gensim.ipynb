{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ritu-95/Machine-Learning-An-Overview/blob/main/LSA_model_using_Gensim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsn6MxPIcTV3"
      },
      "source": [
        "# Latent Semantic Analysis\n",
        "\n",
        "LSA (Latent Semantic Analysis) also known as LSI (Latent Semantic Index) LSA uses bag of words(BoW) model, which results in a term-document matrix(occurrence of terms in a document). Rows represent terms and columns represent documents. LSA learns latent topics by performing a matrix decomposition on the document-term matrix using Singular value decomposition. LSA is typically used as a dimension reduction or noise reducing technique.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATyDJH2BcTV-"
      },
      "source": [
        "Implementing LSA using Gensim\n",
        "\n",
        "Import the required library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WuPSdvtHcTV_"
      },
      "outputs": [],
      "source": [
        "#import modules\n",
        "import os.path\n",
        "from gensim import corpora\n",
        "from gensim.models import LsiModel\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AUqUbf5fcTWA"
      },
      "outputs": [],
      "source": [
        "def load_data(path,file_name):\n",
        "    \"\"\"\n",
        "    Input  : path and file_name\n",
        "    Purpose: loading text file\n",
        "    Output : list of paragraphs/documents and\n",
        "             title(initial 100 words considered as title of document)\n",
        "    \"\"\"\n",
        "    documents_list = []\n",
        "    titles=[]\n",
        "    with open( os.path.join(path, file_name) ,\"r\") as fin:\n",
        "        for line in fin.readlines():\n",
        "            text = line.strip()\n",
        "            documents_list.append(text)\n",
        "    print(\"Total Number of Documents:\",len(documents_list))\n",
        "    titles.append( text[0:min(len(text),100)] )\n",
        "    return documents_list,titles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TS7O28hcTWB"
      },
      "source": [
        "##  Preprocessing Data\n",
        "\n",
        "After data loading function, you need to preprocess the text. Following steps are taken to preprocess the text:\n",
        "\n",
        "- Tokenize the text articles\n",
        "- Remove stop words\n",
        "- Perform stemming in text article\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7IkmlggVcTWC"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(doc_set):\n",
        "    \"\"\"\n",
        "    Input  : document list\n",
        "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
        "    Output : preprocessed text\n",
        "   \"\"\"\n",
        "    # initialize regex tokenizer\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    # create English stop words list\n",
        "    en_stop = set(stopwords.words('english'))\n",
        "    # Create p_stemmer of class PorterStemmer\n",
        "    p_stemmer = PorterStemmer()\n",
        "    # list for tokenized documents in loop\n",
        "    texts = []\n",
        "    # loop through document list\n",
        "    for i in doc_set:\n",
        "        # clean and tokenize document string\n",
        "        raw = i.lower()\n",
        "        tokens = tokenizer.tokenize(raw)\n",
        "        # remove stop words from tokens\n",
        "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
        "        # stem tokens\n",
        "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
        "        # add tokens to list\n",
        "        texts.append(stemmed_tokens)\n",
        "    return texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13fTelRUcTWG"
      },
      "source": [
        "## Prepare Corpus\n",
        "\n",
        "Next step is to prepare corpus. Here, you need to create a document-term matrix and dictionary of terms. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bCRhIuGXcTWG"
      },
      "outputs": [],
      "source": [
        "def prepare_corpus(doc_clean):\n",
        "    \"\"\"\n",
        "    Input  : clean document\n",
        "    Purpose: create term dictionary of our corpus and Converting list of documents (corpus) into Document Term Matrix\n",
        "    Output : term dictionary and Document Term Matrix\n",
        "    \"\"\"\n",
        "    # Creating the term dictionary of our corpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
        "    dictionary = corpora.Dictionary(doc_clean)\n",
        "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "    # generate LDA model\n",
        "    return dictionary,doc_term_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyiWZEdZcTWH"
      },
      "source": [
        "## Create an LSA model using Gensim\n",
        "\n",
        "After corpus creation, you can generate a model using LSA.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9FHEwg5hcTWH"
      },
      "outputs": [],
      "source": [
        "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
        "    \"\"\"\n",
        "    Input  : clean document, number of topics and number of words associated with each topic\n",
        "    Purpose: create LSA model using gensim\n",
        "    Output : return LSA model\n",
        "    \"\"\"\n",
        "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
        "    # generate LSA model\n",
        "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
        "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
        "    return lsamodel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSxlxZ-EcTWI"
      },
      "source": [
        "## Determine the number of topics\n",
        "\n",
        "Another extra step needs to be taken to optimize results by identifying an optimum amount of topics. Here, you will generate coherence scores to determine an optimum number of topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "C0DbpWt0cTWI"
      },
      "outputs": [],
      "source": [
        "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
        "    \"\"\"\n",
        "    Input   : dictionary : Gensim dictionary\n",
        "              corpus : Gensim corpus\n",
        "              texts : List of input texts\n",
        "              stop : Max num of topics\n",
        "    purpose : Compute c_v coherence for various number of topics\n",
        "    Output  : model_list : List of LSA topic models\n",
        "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, stop, step):\n",
        "        # generate LSA model\n",
        "        model = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znBtyRdOcTWJ"
      },
      "source": [
        "Let's plot coherence score values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1fL9eH43cTWJ"
      },
      "outputs": [],
      "source": [
        "def plot_graph(doc_clean,start, stop, step):\n",
        "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
        "    model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix,doc_clean,\n",
        "                                                            stop, start, step)\n",
        "    # Show graph\n",
        "    x = range(start, stop, step)\n",
        "    plt.plot(x, coherence_values)\n",
        "    plt.xlabel(\"Number of Topics\")\n",
        "    plt.ylabel(\"Coherence score\")\n",
        "    plt.legend((\"coherence_values\"), loc='best')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFVKu6LNcTWJ"
      },
      "source": [
        "You can easily evaluate this graph. Here, you have a number of topics on X-axis and coherence score on Y-axis. Of the number of topics, 7 have the highest coherence score, so the optimum number of topics are 7.\n",
        "\n",
        "\n",
        "Run all the above functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qpTDO9dOcTWJ",
        "outputId": "933b8d72-2644-4895-f6af-bd49eb7fb636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-99bb16f69533>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnumber_of_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdocument_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"articles.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdocument_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"articles.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-af440c382e4b>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(path, file_name)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdocuments_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtitles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'articles.txt'"
          ]
        }
      ],
      "source": [
        "# LSA Model\n",
        "number_of_topics=7\n",
        "words=10\n",
        "#document_list,titles=load_data(\"\",\"articles.txt\")\n",
        "document_list = open(\"articles.txt\", encoding=\"utf8\")\n",
        "clean_text=preprocess_data(document_list)\n",
        "model=create_gensim_lsa_model(clean_text,number_of_topics,words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNUaodYncTWK"
      },
      "source": [
        "- Topic 1 : a, trump, say, said, would, peopl, clinton, one, campaign ((US Presidential Elections)\n",
        "- Topic 2: citi, v, h, unit, west, manchest, apr, dec (English Premier League)\n",
        "- Topic 3: eu, trump, say, a would, leav, uk, clinton, said, vote (US Presidential Elections,)\n",
        "- Topic 4: trump, min, clinton, said, campaign, eu, vote, say, goal (US Presidential Elections, EPL)\n",
        "- Topic 5: min, trump, clinton, goal, ball, 1, 0, win, leagu (US Presidential Elections, EPL)\n",
        "- Topic 6: bank, eu, say, min, market, year, rate, leav, financi, cameron (Brexit and Market Condition)\n",
        "- Topic 7: say, eu, said, vote, poll, campaign, govern, remain, leav, tax (US Presidential Elections and Financial Planning)\n",
        "\n",
        "Here, 7 Topics were discovered using Latent Semantic Analysis. Some of them are overlapping topics. For Capturing multiple meanings with higher accuracy we need to try LDA( latent Dirichlet allocation). \n",
        "\n",
        "\n",
        "I will leave this as an exercise for you, try it out using Gensim and share your views.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}